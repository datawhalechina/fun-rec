{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 点击率预估简介\n",
    "\n",
    "**点击率预估是用来解决什么问题？**\n",
    "\n",
    "点击率预估是对每次广告点击情况作出预测，可以输出点击或者不点击，**也可以输出该次点击的概率**，后者有时候也称为pClick.\n",
    "\n",
    "<br>\n",
    "\n",
    "**点击率预估模型需要做什么？**\n",
    "\n",
    "通过上述点击率预估的基本概念，我们会发现其实点击率预估问题就是一个二分类的问题，在机器学习中可以使用逻辑回归作为模型的输出，其输出的就是一个概率值，**我们可以将机器学习输出的这个概率值认为是某个用户点击某个广告的概率**。\n",
    "\n",
    "<br>\n",
    "\n",
    "**点击率预估与推荐算法有什么不同？**\n",
    "\n",
    "广告点击率预估是需要得到某个用户对某个广告的点击率，**然后结合广告的出价用于排序**；而推荐算法很多大多数情况下只需要得到一个最优的推荐次序，即TopN推荐的问题。当然也可以利用广告的点击率来排序，作为广告的推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FM它不香吗\n",
    "\n",
    "之前我们已经学了FM模型，不是已经很好了吗，为啥还要整这个Wide&Deep呢？其缺点在于：当query-item矩阵是稀疏并且是high-rank的时候（比如user有特殊的爱好，或item比较小众），很难非常效率的学习出低维度的表示。这种情况下，大部分的query-item都没有什么关系。但是dense embedding会导致几乎所有的query-item预测值都是非0的，这就导致了推荐过度泛化，会推荐一些不那么相关的物品。相反，简单的linear model却可以通过cross-product transformation来记住这些**exception rules**，cross-product transformation是什么意思后面再提。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wide & Deep模型的“记忆能力”与“泛化能力”\n",
    "\n",
    "Memorization 和 Generalization是推荐系统很常见的两个概念，其中Memorization指的是通过用户与商品的交互信息矩阵学习规则，而Generalization则是泛化规则。我们前面介绍的FM算法就是很好的Generalization的例子，它可以根据交互信息学习到一个比较短的矩阵$V$，其中$v_{i}$储存着每个用户特征的压缩表示（embedding），而协同过滤与SVD都是靠记住用户之前与哪些物品发生了交互从而推断出的推荐结果，这两者推荐结果当然存在一些差异，我们的Wide&Deep模型就能够融合这两种推荐结果做出最终的推荐，得到一个比之前的推荐结果都好的模型。\n",
    "\n",
    "可以这么说：Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。Memorization只需要使用一个线性模型即可实现，而Generalization需要使用DNN实现。\n",
    "\n",
    "下面是wide&deep模型的结构图，由左边的wide部分(一个简单的线性模型)，右边的deep部分(一个典型的DNN模型)。\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20200910214310877.png\" alt=\"image-20200910214310877\" style=\"zoom:65%;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "其实wide&deep模型本身的结构是非常简单的，对于有点机器学习基础和深度学习基础的人来说都非常的容易看懂，**但是如何根据自己的场景去选择那些特征放在Wide部分，哪些特征放在Deep部分就需要理解这篇论文提出者当时对于设计该模型不同结构时的意图了**，所以这也是用好这个模型的一个前提。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何理解Wide部分有利于增强模型的“记忆能力”，Deep部分有利于增强模型的“泛化能力”？**\n",
    "\n",
    "- wide部分是一个广义的线性模型，输入的特征主要有两部分组成，一部分是原始的部分特征，另一部分是原始特征的交互特征(cross-product transformation)，对于交互特征可以定义为：\n",
    "  $$\n",
    "  \\phi_{k}(x)=\\prod_{i=1}^d x_i^{c_{ki}}, c_{ki}\\in \\{0,1\\}\n",
    "  $$\n",
    "  这个式子什么意思读者可以自行找原论文看看，大体意思就是两个特征都同时为1这个新的特征才能为1，否则就是0，说白了就是一个特征组合。用原论文的例子举例：\n",
    "\n",
    "  > AND(user_installed_app=QQ, impression_app=WeChat)，当特征user_installed_app=QQ,和特征impression_app=WeChat取值都为1的时候，组合特征AND(user_installed_app=QQ, impression_app=WeChat)的取值才为1，否则为0。\n",
    "\n",
    "  对于wide部分训练时候使用的优化器是带$L_1$正则的FTRL算法(Follow-the-regularized-leader)，而L1 FTLR是非常注重模型稀疏性质的，也就是说W&D模型采用L1 FTRL是想让Wide部分变得更加的稀疏，即Wide部分的大部分参数都为0，这就大大压缩了模型权重及特征向量的维度。**Wide部分模型训练完之后留下来的特征都是非常重要的，那么模型的“记忆能力”就可以理解为发现\"直接的\"，“暴力的”，“显然的”关联规则的能力。**例如Google W&D期望wide部分发现这样的规则：**用户安装了应用A，此时曝光应用B，用户安装应用B的概率大。**\n",
    "\n",
    "- Deep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征(可直接输入DNN)，一类是类别特征(需要经过Embedding之后才能输入到DNN中)，Deep部分的数学形式如下：\n",
    "  $$\n",
    "  a^{(l+1)} = f(W^{l}a^{(l)} + b^{l})\n",
    "  $$\n",
    "  **我们知道DNN模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。**对于Deep部分的DNN模型作者使用了深度学习常用的优化器AdaGrad，这也是为了使得模型可以得到更精确的解。\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wide部分与Deep部分的结合**\n",
    "\n",
    "W&D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：\n",
    "$$\n",
    "P(Y=1|x)=\\delta(w_{wide}^T[x,\\phi(x)] + w_{deep}^T a^{(lf)} + b)\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 操作流程\n",
    "\n",
    "* **Retrieval **：利用机器学习模型和一些人为定义的规则，来返回最匹配当前Query的一个小的items集合，这个集合就是最终的推荐列表的候选集。\n",
    "\n",
    "* **Ranking**：\n",
    "  * 收集更细致的用户特征，如：\n",
    "    - User features（年龄、性别、语言、民族等）\n",
    "    - Contextual features(上下文特征：设备，时间等)\n",
    "    - Impression features（展示特征：app age、app的历史统计信息等）\n",
    "  * 将特征分别传入Wide和Deep**一起做训练**。在训练的时候，根据最终的loss计算出gradient，反向传播到Wide和Deep两部分中，分别训练自己的参数（wide组件只需要填补deep组件的不足就行了，所以需要比较少的cross-product feature transformations，而不是full-size wide Model）\n",
    "    * 训练方法是用mini-batch stochastic optimization。\n",
    "    * Wide组件是用FTRL（Follow-the-regularized-leader） + L1正则化学习。\n",
    "    * Deep组件是用AdaGrad来学习。\n",
    "  * 训练完之后推荐TopN\n",
    "\n",
    "**所以wide&deep模型尽管在模型结构上非常的简单，但是如果想要很好的使用wide&deep模型的话，还是要深入理解业务，确定wide部分使用哪部分特征，deep部分使用哪些特征，以及wide部分的交叉特征应该如何去选择**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 代码实战\n",
    "\n",
    "代码实战主要分为两大部分，第一部分是使用tensorflow中已经封装好的wide&deep模型，这一部分主要是熟悉模型训练的整体结构。第二部分是使用tensorflow中的keras实现wide&deep，这一部分主要是尽可能的看到模型内部的细节并将其实现。\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tensorflow内置的WideDeepModel**\n",
    "\n",
    "在Tensorflow的库中是已经内置了Wide-Deep model的，想要查看源代码了解具体实现过程可以看[这里](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/premade/wide_deep.py#L34-L219)。下面参考[Tensorflow官网的示例代码](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel)进行讲解。我们用到的数据集下载链接[戳这里](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random, math, os\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先看全局实现：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，并将标签做简单的转换\n",
    "def get_data():\n",
    "    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "               \"income_bracket\"]\n",
    "\n",
    "    df_train = pd.read_csv(\"./adult_train.csv\", names=COLUMNS)\n",
    "    df_test = pd.read_csv(\"./adult_test.csv\", names=COLUMNS)\n",
    "\n",
    "    df_train['income_label'] = (df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)  # 返回 1 或者 0\n",
    "    df_test['income_label'] = (df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
       "       'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
       "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
       "       'income_bracket', 'income_label', 'age_group', 'IS_TRAIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征处理分为wide部分的特征处理和deep部分的特征处理\n",
    "def data_process(df_train, df_test):\n",
    "    # 年龄特征离散化\n",
    "    age_groups = [0, 25, 65, 90]\n",
    "    age_labels = range(len(age_groups) - 1)\n",
    "    df_train['age_group'] = pd.cut(df_train['age'], age_groups, labels=age_labels)\n",
    "    df_test['age_group'] = pd.cut(df_test['age'], age_groups, labels=age_labels)\n",
    "\n",
    "    # wide部分的原始特征及交叉特征\n",
    "    wide_cols = ['workclass', 'education', 'marital_status', 'occupation', \\\n",
    "                 'relationship', 'race', 'gender', 'native_country', 'age_group']\n",
    "    x_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "    # deep部分的特征分为两大类，一类是数值特征(可以直接输入到网络中进行训练)，\n",
    "    # 一类是类别特征(只能在embedding之后才能输入到模型中进行训练）\n",
    "    embedding_cols = ['workclass', 'education', 'marital_status', 'occupation', \\\n",
    "                      'relationship', 'race', 'gender', 'native_country']\n",
    "    cont_cols = ['age', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "    # 类别标签\n",
    "    target = 'income_label'\n",
    "\n",
    "    return df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass  fnlwgt   education  education_num  marital_status  \\\n",
       "0   39   State-gov   77516   Bachelors             13   Never-married   \n",
       "\n",
       "      occupation    relationship    race gender  capital_gain  capital_loss  \\\n",
       "0   Adm-clerical   Not-in-family   White   Male          2174             0   \n",
       "\n",
       "   hours_per_week  native_country income_bracket  income_label  \n",
       "0              40   United-States          <=50K             0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt education  education_num  marital_status  \\\n",
       "0   25   Private  226802      11th              7   Never-married   \n",
       "\n",
       "           occupation relationship    race gender  capital_gain  capital_loss  \\\n",
       "0   Machine-op-inspct    Own-child   Black   Male             0             0   \n",
       "\n",
       "   hours_per_week  native_country income_bracket  income_label  \n",
       "0              40   United-States         <=50K.             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的目的，就是实现 wide 部分的特征，以及交叉特征的组合。\n",
    "# 思考：wide 部分哪些字段用来交叉，这是需要考虑的事情。如果没有想法的话，拿出所有 分类特征 进行交叉。\n",
    "def process_wide_feats(df_train, df_test, wide_cols, x_cols, target):\n",
    "    # 合并训练和测试数据，后续一起编码\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_wide = pd.concat([df_train, df_test])\n",
    "\n",
    "    # 选出wide部分特征中的类别特征, 类别特征在DataFrame中是object类型\n",
    "    categorical_columns = list(df_wide.select_dtypes(include=['object']).columns) \n",
    "\n",
    "    # 构造交叉特征\n",
    "    crossed_columns_d = []\n",
    "    for f1, f2 in x_cols:\n",
    "        col_name = f1 + '_' + f2\n",
    "        crossed_columns_d.append(col_name)\n",
    "        df_wide[col_name] = df_wide[[f1, f2]].apply(lambda x: '-'.join(x), axis=1)\n",
    "        \n",
    "    # wide部分的所有特征\n",
    "    wide_cols += crossed_columns_d\n",
    "    df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
    "\n",
    "    # 将wide部分类别特征进行onehot编码\n",
    "    dummy_cols = [c for c in wide_cols if c in categorical_columns + crossed_columns_d]\n",
    "    df_wide = pd.get_dummies(df_wide, columns=[x for x in dummy_cols])\n",
    "\n",
    "    # 将训练数据和测试数据分离\n",
    "    train = df_wide[df_wide.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_wide[df_wide.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    cols = [c for c in train.columns if c != target]\n",
    "    X_train = train[cols].values\n",
    "    y_train = train[target].values.reshape(-1, 1)\n",
    "    X_test = test[cols].values\n",
    "    y_test = test[target].values.reshape(-1, 1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, target = data_process(df_train, df_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = process_wide_feats(df_train, df_test, wide_cols, x_cols, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.experimental.WideDeepModel(\n",
    "#     linear_model, dnn_model, activation=None, **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep部分的特征组合，构建\n",
    "def process_deep_feats(df_train, df_test, embedding_cols, cont_cols, target, emb_dim=8, emb_reg=1e-3):\n",
    "    # 标记训练和测试集，方便特征处理完之后进行训练和测试集的分离\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_deep = pd.concat([df_train, df_test])\n",
    "\n",
    "    # 拼接数值特征和embedding特征\n",
    "    deep_cols = embedding_cols + cont_cols\n",
    "    df_deep = df_deep[deep_cols + [target,'IS_TRAIN']]\n",
    "\n",
    "    # 数值类特征进行标准化\n",
    "    scaler = StandardScaler()\n",
    "    df_deep[cont_cols] = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]), columns=cont_cols)\n",
    "\n",
    "    # 类边特征编码\n",
    "    unique_vals = dict()  #记录每个字段，有几个不同类别\n",
    "    lbe = LabelEncoder()\n",
    "    for feats in embedding_cols:\n",
    "        df_deep[feats] = lbe.fit_transform(df_deep[feats])\n",
    "        unique_vals[feats] = df_deep[feats].nunique()\n",
    "\n",
    "    # 构造模型的输入层，和embedding层，虽然对于连续的特征没有embedding层，但是为了统一，将Reshape层\n",
    "    # 当成是连续特征的embedding层\n",
    "    inp_layer = []\n",
    "    emb_layer = []\n",
    "    for ec in embedding_cols:\n",
    "        layer_name = ec + '_inp'\n",
    "        inp = Input(shape=(1,), dtype='int64', name=layer_name)\n",
    "        emb = Embedding(unique_vals[ec], emb_dim, input_length=1, embeddings_regularizer=l2(emb_reg))(inp)\n",
    "        inp_layer.append(inp)\n",
    "        emb_layer.append(inp)\n",
    "\n",
    "    for cc in cont_cols:\n",
    "        layer_name = cc + '_inp'\n",
    "        inp = Input(shape=(1,), dtype='int64', name=layer_name)\n",
    "        emb = Reshape((1, 1))(inp)\n",
    "        inp_layer.append(inp)\n",
    "        emb_layer.append(inp)\n",
    "\n",
    "    # 训练和测试集分离\n",
    "    train = df_deep[df_deep.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_deep[df_deep.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    # 提取训练和测试集中的特征\n",
    "    X_train = [train[c] for c in deep_cols]\n",
    "    y_train = np.array(train[target].values).reshape(-1, 1)\n",
    "    X_test = [test[c] for c in deep_cols]\n",
    "    y_test = np.array(test[target].values).reshape(-1, 1)\n",
    "\n",
    "    # 返回构建好的输入层和embedding层\n",
    "    return X_train, y_train, X_test, y_test, emb_layer, inp_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols):\n",
    "    # wide部分特征处理\n",
    "    X_train_wide, y_train_wide, X_test_wide, y_test_wide = \\\n",
    "        process_wide_feats(df_train, df_test, wide_cols, x_cols, target)\n",
    "\n",
    "    # deep部分特征处理\n",
    "    X_train_deep, y_train_deep, X_test_deep, y_test_deep, deep_inp_embed, deep_inp_layer = \\\n",
    "        process_deep_feats(df_train, df_test, embedding_cols,cont_cols, target)\n",
    "\n",
    "    # wide特征与deep特征拼接\n",
    "    X_tr_wd = [X_train_wide] + X_train_deep\n",
    "    Y_tr_wd = y_train_deep  # wide部分和deep部分的label是一样的\n",
    "    X_te_wd = [X_test_wide] + X_test_deep\n",
    "    Y_te_wd = y_test_deep  # wide部分和deep部分的label是一样的\n",
    "\n",
    "    # wide部分的输入\n",
    "    w = Input(shape=(X_train_wide.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "    # deep部分的NN结构\n",
    "    # 这里我目前有些不明白 2021.03.09\n",
    "    deep_inp_embed = list(pd.Series(deep_inp_embed).astype('float'))  # 列表数据 转为 float 形式\n",
    "    d = concatenate(deep_inp_embed)\n",
    "    d = Flatten()(d)\n",
    "    d = Dense(50, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "    d = Dropout(0.5)(d)\n",
    "    d = Dense(20, activation='relu', name='deep')(d)\n",
    "    d = Dropout(0.5)(d)\n",
    "\n",
    "    # 将wide部分与deep部分的输入进行拼接, 然后输入一个线性层\n",
    "    wd_inp = concatenate([w, d])\n",
    "    wd_out = Dense(Y_tr_wd.shape[1], activation='sigmoid', name='wide_deep')(wd_inp) \n",
    "    \n",
    "    # 构建模型，这里需要注意模型的输入部分是由wide和deep部分组成的\n",
    "    wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "    wide_deep.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "    # 设置模型学习率，不设置学习率keras默认的学习率是0.01\n",
    "    wide_deep.optimizer.lr = 0.001\n",
    "\n",
    "    # 模型训练\n",
    "    wide_deep.fit(X_tr_wd, Y_tr_wd, epochs=5, batch_size=128)\n",
    "\n",
    "    # 模型预测及验证\n",
    "    results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "    print(\"\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.9 GiB for an array with shape (57289, 32561) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\MySoftware\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\callbacks\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwide_deep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwide_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\MySoftware\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\callbacks\\__init__.py\u001b[0m in \u001b[0;36mwide_deep\u001b[1;34m(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# wide部分特征处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_train_wide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_wide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_wide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_wide\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mprocess_wide_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwide_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# deep部分特征处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MySoftware\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\callbacks\\__init__.py\u001b[0m in \u001b[0;36mprocess_wide_feats\u001b[1;34m(df_train, df_test, wide_cols, x_cols, target)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \"\"\"\n\u001b[0;32m   5339\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m    851\u001b[0m                     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m             \u001b[1;31m# The underlying data was copied within _interleave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"object\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[0mitemmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.9 GiB for an array with shape (57289, 32561) and data type int64"
     ]
    }
   ],
   "source": [
    "wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 读取数据\n",
    "    df_train, df_test = get_data()\n",
    "\n",
    "    # 特征处理\n",
    "    df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, target = data_process(df_train, df_test)\n",
    "\n",
    "    # 模型训练\n",
    "    wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
