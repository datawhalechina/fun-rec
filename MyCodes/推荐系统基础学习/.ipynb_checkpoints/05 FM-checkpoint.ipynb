{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FM模型的引入\n",
    "\n",
    "#### 1.1 逻辑回归模型及其缺点\n",
    "\n",
    "FM模型其实是一种思路，具体的应用稍少。一般来说做推荐CTR预估时最简单的思路就是将特征做线性组合（逻辑回归LR），传入sigmoid中得到一个概率值，本质上这就是一个线性模型，因为sigmoid是单调增函数不会改变里面的线性模型的CTR预测顺序，因此逻辑回归模型效果会比较差。也就是LR的缺点有：\n",
    "\n",
    "* 是一个线性模型\n",
    "* 每个特征对最终输出结果独立，需要手动特征交叉（$x_i*x_j$），比较麻烦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上做的唯一改动就是把$w_{ij}$替换成了$\\lt v_i,v_j\\gt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 二阶交叉项的考虑及改进\n",
    "\n",
    "由于LR模型的上述缺陷（主要是手动做特征交叉比较麻烦），干脆就考虑所有的二阶交叉项，也就是将目标函数由原来的\n",
    "\n",
    "$$\n",
    "y = w_0+\\sum_{i=1}^nw_ix_i\n",
    "$$\n",
    "变为\n",
    "\n",
    "$$\n",
    "y = w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^{n-1}\\sum_{i+1}^nw_{ij}x_ix_j\n",
    "$$\n",
    "但这个式子有一个问题，**只有当$x_i$与$x_j$均不为0时这个二阶交叉项才会生效**，**后面这个特征交叉项本质是和多项式核SVM等价的，为了解决这个问题，我们的FM登场了！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这个式子有一个问题，**只有当$x_i$与$x_j$均不为0时这个二阶交叉项才会生效**，后面这个特征交叉项本质是和多项式核SVM等价的，为了解决这个问题，我们的FM登场了！\n",
    "\n",
    "FM模型使用了如下的优化函数：\n",
    "\n",
    "$$\n",
    "y = w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^{n}\\sum_{i+1}^n\\lt v_i,v_j\\gt x_ix_j\n",
    "$$\n",
    "事实上做的唯一改动就是把$w_{ij}$替换成了$\\lt v_i,v_j\\gt$，大家应该就看出来了，这实际上就有深度学习的意味在里面了，实质上就是给每个$x_i$计算一个embedding，然后将两个向量之间的embedding做内积得到之前所谓的$w_{ij}$好处就是这个模型泛化能力强 ，即使两个特征之前从未在训练集中**同时**出现，我们也不至于像之前一样训练不出$w_{ij}$，事实上只需要$x_i$和其他的$x_k$同时出现过就可以计算出$x_i$的embedding！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FM公式的理解\n",
    "\n",
    "从公式来看，模型前半部分就是普通的LR线性组合，后半部分的交叉项：特征组合。首先，单从模型表达能力上来看，FM是要强于LR的，至少它不会比LR弱，当交叉项参数$w_{ij}$全为0的时候，整个模型就退化为普通的LR模型。对于有$n$个特征的模型，特征组合的参数数量共有$1+2+3+\\cdots  + n-1=\\frac{n(n-1)}{2}$个，并且任意两个参数之间是独立的。所以说特征数量比较多的时候，特征组合之后，维度自然而然就高了。\n",
    "\n",
    "> 定理：任意一个实对称矩阵（正定矩阵）$W$都存在一个矩阵$V$，使得 $W=V.V^{T}$成立。\n",
    "\n",
    "类似地，所有二次项参数$\\omega_{ij}$可以组成一个对称阵$W$（为了方便说明FM的由来，对角元素可以设置为正实数），那么这个矩阵就可以分解为$W=V^TV$，$V$ 的第$j$列($v_{j}$)便是第$j$维特征($x_{j}$)的隐向量。\n",
    "\n",
    "$$\n",
    "\\hat{y}(X) = \\omega_{0}+\\sum_{i=1}^{n}{\\omega_{i}x_{i}}+\\sum_{i=1}^{n-1}{\\sum_{j=i+1}^{n} \\color{red}{<v_{i},v_{j}>x_{i}x_{j}}}\n",
    "$$\n",
    "\n",
    "需要估计的参数有$\\omega_{0}∈ R$，$\\omega_{i}∈ R$，$V∈ R$，$< \\cdot, \\cdot>$是长度为$k$的两个向量的点乘，公式如下：\n",
    "\n",
    "$$\n",
    "<v_{i},v_{j}> = \\sum_{f=1}^{k}{v_{i,f}\\cdot v_{j,f}}\n",
    "$$\n",
    "\n",
    "上面的公式中： \n",
    "- $\\omega_{0}$为全局偏置；\n",
    "- $\\omega_{i}$是模型第$i$个变量的权重;\n",
    "- $\\omega_{ij} = < v_{i}, v_{j}>$特征$i$和$j$的交叉权重;\n",
    "- $v_{i} $是第$i$维特征的隐向量;\n",
    "- $<\\cdot, \\cdot>$代表向量点积;\n",
    "- $k(k<<n)$为隐向量的长度，包含 $k$ 个描述特征的因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM模型中二次项的参数数量减少为 $kn $个，远少于多项式模型的参数数量。另外，参数因子化使得 $x_{h}x_{i}$ 的参数和 $x_{i}x_{j}$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，$x_{h}x_{i}$ 和 $x_{i}x_{j}$的系数分别为 $\\lt v_{h},v_{i}\\gt$ 和 $\\lt v_{i},v_{j}\\gt$ ，它们之间有共同项 $v_{i}$ 。也就是说，**所有包含“ $x_{i}$ 的非零组合特征”（存在某个 $j \\ne i$ ，使得 $x_{i}x_{j}\\neq 0$ ）的样本都可以用来学习隐向量$v_{i}$**，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中,$w_{hi}$ 和 $w_{ij}$ 是相互独立的。\n",
    "\n",
    "显而易见，FM的公式是一个通用的拟合方程，可以采用不同的损失函数用于解决regression、classification等问题，比如可以采用MSE（Mean Square Error）loss function来求解回归问题，也可以采用Hinge/Cross-Entropy loss来求解分类问题。当然，在进行二元分类时，FM的输出需要使用sigmoid函数进行变换，该原理与LR是一样的。直观上看，FM的复杂度是 $O(kn^2)$ 。但是FM的二次项可以化简，其复杂度可以优化到 $O(kn)$ 。由此可见，FM可以在线性时间对新样本作出预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FM模型的应用\n",
    "\n",
    "最直接的想法就是直接把FM得到的结果放进sigmoid中输出一个概率值，由此做CTR预估，事实上我们也可以做**召回**。\n",
    "\n",
    "由于FM模型是利用两个特征的Embedding做内积得到二阶特征交叉的权重，那么我们可以将训练好的FM特征取出离线存好，之后用来做KNN向量检索。\n",
    "\n",
    "工业应用的具体操作步骤：\n",
    "\n",
    "* 离线训练好FM模型（学习目标可以是CTR）\n",
    "* 将训练好的FM模型Embedding取出\n",
    "* 将每个uid对应的Embedding做avg pooling（平均）形成该用户最终的Embedding，item也做同样的操作\n",
    "* 将所有的Embedding向量放入Faiss等\n",
    "* 线上uid发出请求，取出对应的user embedding，进行检索召回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 代码实践\n",
    "\n",
    "#### 4.1 调包实现\n",
    "\n",
    "**调包版**\n",
    "\n",
    "直接看Github官方仓库：https://github.com/coreylynch/pyFM，里面有介绍如何安装以及使用，下面搬运一遍：\n",
    "\n",
    "**安装**\n",
    "\n",
    "**方法一：直接pip install**\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/coreylynch/pyFM\n",
    "```\n",
    "\n",
    "**方法二：手动安装**\n",
    "\n",
    "输入上面这行代码应能下载这个包并安装，如果安装失败可能是网络原因，这时可以考虑手动下载这个包然后手动`python setup.py install`安装，这时候通常会报错，去掉setup.py文件里面的`libraries=[“m”]`一行再重新安装即可\n",
    "\n",
    "具体操作是：\n",
    "\n",
    "* 在https://github.com/coreylynch/pyFM中手动下载包\n",
    "* 将包解压，更改里面的setup.py文件，去掉setup.py文件里面的`libraries=[“m”]`一行\n",
    "* cd到当前文件夹下`python setup.py install`\n",
    "\n",
    "**测试**\n",
    "\n",
    "这部分主要作为简单上手让读者了解如何使用这个包~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：导包**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfm import pylibfm\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：创建训练集并转换成one-hot编码的特征形式** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.  0.  0.  0.  1.  1.  0.  0.  0.]\n",
      " [33.  0.  0.  1.  0.  0.  1.  0.  0.]\n",
      " [55.  0.  1.  0.  0.  0.  0.  1.  0.]\n",
      " [20.  1.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "train = [\n",
    "    {\"user\": \"1\", \"item\": \"5\", \"age\": 19},\n",
    "    {\"user\": \"2\", \"item\": \"43\", \"age\": 33},\n",
    "    {\"user\": \"3\", \"item\": \"20\", \"age\": 55},\n",
    "    {\"user\": \"4\", \"item\": \"10\", \"age\": 20},\n",
    "]\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(train)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步：创建标签**\n",
    "\n",
    "这里简单创建了一个全1的标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.repeat(1.0,X.shape[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第四步：训练并预测**\n",
    "\n",
    "就和调用`sklearn`的包是一样的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training log loss: 0.16600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99008556])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = pylibfm.FM()\n",
    "fm.fit(X,y)\n",
    "fm.predict(v.transform({\"user\": \"1\", \"item\": \"10\", \"age\": 24}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **电影评分数据集实战**\n",
    "\n",
    "数据集在这里下载，数据集本地具体保存路径读者自行阅读代码找找： http://www.grouplens.org/system/files/ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**导包，并定义一个导入指定格式数据集的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pyfm import pylibfm\n",
    "\n",
    "# Read in data\n",
    "def loadData(filename,path=\"ml-100k/\"):\n",
    "    data = []\n",
    "    y = []\n",
    "    users=set()\n",
    "    items=set()\n",
    "    with open(path+filename) as f:\n",
    "        for line in f:\n",
    "            (user,movieid,rating,ts)=line.split('\\t')\n",
    "            data.append({ \"user_id\": str(user), \"movie_id\": str(movieid)})\n",
    "            y.append(float(rating))\n",
    "            users.add(user)\n",
    "            items.add(movieid)\n",
    "\n",
    "    return (data, np.array(y), users, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**导入训练集和测试集，并转换格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, y_train, train_users, train_items) = loadData(\"ua.base\")\n",
    "(test_data, y_test, test_users, test_items) = loadData(\"ua.test\")\n",
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(train_data)\n",
    "X_test = v.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练模型并测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training MSE: 0.59525\n",
      "-- Epoch 2\n",
      "Training MSE: 0.51804\n",
      "-- Epoch 3\n",
      "Training MSE: 0.49046\n",
      "-- Epoch 4\n",
      "Training MSE: 0.47458\n",
      "-- Epoch 5\n",
      "Training MSE: 0.46416\n",
      "-- Epoch 6\n",
      "Training MSE: 0.45662\n",
      "-- Epoch 7\n",
      "Training MSE: 0.45099\n",
      "-- Epoch 8\n",
      "Training MSE: 0.44639\n",
      "-- Epoch 9\n",
      "Training MSE: 0.44264\n",
      "-- Epoch 10\n",
      "Training MSE: 0.43949\n",
      "-- Epoch 11\n",
      "Training MSE: 0.43675\n",
      "-- Epoch 12\n",
      "Training MSE: 0.43430\n",
      "-- Epoch 13\n",
      "Training MSE: 0.43223\n",
      "-- Epoch 14\n",
      "Training MSE: 0.43020\n",
      "-- Epoch 15\n",
      "Training MSE: 0.42851\n",
      "-- Epoch 16\n",
      "Training MSE: 0.42691\n",
      "-- Epoch 17\n",
      "Training MSE: 0.42531\n",
      "-- Epoch 18\n",
      "Training MSE: 0.42389\n",
      "-- Epoch 19\n",
      "Training MSE: 0.42255\n",
      "-- Epoch 20\n",
      "Training MSE: 0.42128\n",
      "-- Epoch 21\n",
      "Training MSE: 0.42003\n",
      "-- Epoch 22\n",
      "Training MSE: 0.41873\n",
      "-- Epoch 23\n",
      "Training MSE: 0.41756\n",
      "-- Epoch 24\n",
      "Training MSE: 0.41634\n",
      "-- Epoch 25\n",
      "Training MSE: 0.41509\n",
      "-- Epoch 26\n",
      "Training MSE: 0.41391\n",
      "-- Epoch 27\n",
      "Training MSE: 0.41274\n",
      "-- Epoch 28\n",
      "Training MSE: 0.41149\n",
      "-- Epoch 29\n",
      "Training MSE: 0.41032\n",
      "-- Epoch 30\n",
      "Training MSE: 0.40891\n",
      "-- Epoch 31\n",
      "Training MSE: 0.40774\n",
      "-- Epoch 32\n",
      "Training MSE: 0.40635\n",
      "-- Epoch 33\n",
      "Training MSE: 0.40495\n",
      "-- Epoch 34\n",
      "Training MSE: 0.40354\n",
      "-- Epoch 35\n",
      "Training MSE: 0.40203\n",
      "-- Epoch 36\n",
      "Training MSE: 0.40047\n",
      "-- Epoch 37\n",
      "Training MSE: 0.39889\n",
      "-- Epoch 38\n",
      "Training MSE: 0.39728\n",
      "-- Epoch 39\n",
      "Training MSE: 0.39562\n",
      "-- Epoch 40\n",
      "Training MSE: 0.39387\n",
      "-- Epoch 41\n",
      "Training MSE: 0.39216\n",
      "-- Epoch 42\n",
      "Training MSE: 0.39030\n",
      "-- Epoch 43\n",
      "Training MSE: 0.38847\n",
      "-- Epoch 44\n",
      "Training MSE: 0.38655\n",
      "-- Epoch 45\n",
      "Training MSE: 0.38461\n",
      "-- Epoch 46\n",
      "Training MSE: 0.38269\n",
      "-- Epoch 47\n",
      "Training MSE: 0.38068\n",
      "-- Epoch 48\n",
      "Training MSE: 0.37864\n",
      "-- Epoch 49\n",
      "Training MSE: 0.37657\n",
      "-- Epoch 50\n",
      "Training MSE: 0.37459\n",
      "-- Epoch 51\n",
      "Training MSE: 0.37253\n",
      "-- Epoch 52\n",
      "Training MSE: 0.37045\n",
      "-- Epoch 53\n",
      "Training MSE: 0.36845\n",
      "-- Epoch 54\n",
      "Training MSE: 0.36647\n",
      "-- Epoch 55\n",
      "Training MSE: 0.36448\n",
      "-- Epoch 56\n",
      "Training MSE: 0.36254\n",
      "-- Epoch 57\n",
      "Training MSE: 0.36067\n",
      "-- Epoch 58\n",
      "Training MSE: 0.35874\n",
      "-- Epoch 59\n",
      "Training MSE: 0.35690\n",
      "-- Epoch 60\n",
      "Training MSE: 0.35511\n",
      "-- Epoch 61\n",
      "Training MSE: 0.35333\n",
      "-- Epoch 62\n",
      "Training MSE: 0.35155\n",
      "-- Epoch 63\n",
      "Training MSE: 0.34992\n",
      "-- Epoch 64\n",
      "Training MSE: 0.34829\n",
      "-- Epoch 65\n",
      "Training MSE: 0.34675\n",
      "-- Epoch 66\n",
      "Training MSE: 0.34538\n",
      "-- Epoch 67\n",
      "Training MSE: 0.34393\n",
      "-- Epoch 68\n",
      "Training MSE: 0.34258\n",
      "-- Epoch 69\n",
      "Training MSE: 0.34129\n",
      "-- Epoch 70\n",
      "Training MSE: 0.34006\n",
      "-- Epoch 71\n",
      "Training MSE: 0.33885\n",
      "-- Epoch 72\n",
      "Training MSE: 0.33773\n",
      "-- Epoch 73\n",
      "Training MSE: 0.33671\n",
      "-- Epoch 74\n",
      "Training MSE: 0.33564\n",
      "-- Epoch 75\n",
      "Training MSE: 0.33468\n",
      "-- Epoch 76\n",
      "Training MSE: 0.33375\n",
      "-- Epoch 77\n",
      "Training MSE: 0.33292\n",
      "-- Epoch 78\n",
      "Training MSE: 0.33211\n",
      "-- Epoch 79\n",
      "Training MSE: 0.33131\n",
      "-- Epoch 80\n",
      "Training MSE: 0.33065\n",
      "-- Epoch 81\n",
      "Training MSE: 0.33002\n",
      "-- Epoch 82\n",
      "Training MSE: 0.32930\n",
      "-- Epoch 83\n",
      "Training MSE: 0.32882\n",
      "-- Epoch 84\n",
      "Training MSE: 0.32813\n",
      "-- Epoch 85\n",
      "Training MSE: 0.32764\n",
      "-- Epoch 86\n",
      "Training MSE: 0.32722\n",
      "-- Epoch 87\n",
      "Training MSE: 0.32677\n",
      "-- Epoch 88\n",
      "Training MSE: 0.32635\n",
      "-- Epoch 89\n",
      "Training MSE: 0.32591\n",
      "-- Epoch 90\n",
      "Training MSE: 0.32550\n",
      "-- Epoch 91\n",
      "Training MSE: 0.32513\n",
      "-- Epoch 92\n",
      "Training MSE: 0.32481\n",
      "-- Epoch 93\n",
      "Training MSE: 0.32451\n",
      "-- Epoch 94\n",
      "Training MSE: 0.32421\n",
      "-- Epoch 95\n",
      "Training MSE: 0.32397\n",
      "-- Epoch 96\n",
      "Training MSE: 0.32363\n",
      "-- Epoch 97\n",
      "Training MSE: 0.32341\n",
      "-- Epoch 98\n",
      "Training MSE: 0.32319\n",
      "-- Epoch 99\n",
      "Training MSE: 0.32293\n",
      "-- Epoch 100\n",
      "Training MSE: 0.32268\n"
     ]
    }
   ],
   "source": [
    "# Build and train a Factorization Machine\n",
    "fm = pylibfm.FM(num_factors=10, num_iter=100, verbose=True, task=\"regression\", initial_learning_rate=0.001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测结果打印误差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM MSE: 0.8873\n"
     ]
    }
   ],
   "source": [
    "preds = fm.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"FM MSE: %.4f\" % mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分类任务实战\n",
    "\n",
    "**搞数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyfm import pylibfm\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000,n_features=100, n_clusters_per_class=1)\n",
    "data = [ {v: k for k, v in dict(zip(i, range(len(i)))).items()}  for i in X]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)\n",
    "\n",
    "v = DictVectorizer()\n",
    "X_train = v.fit_transform(X_train)\n",
    "X_test = v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1.2868288212895018,\n",
       " 1: -1.5126510426071544,\n",
       " 2: -2.253987605480358,\n",
       " 3: 0.4141407327558281,\n",
       " 4: 0.39502675672171333,\n",
       " 5: -0.5344973280069945,\n",
       " 6: 0.3334290803877628,\n",
       " 7: -0.45700123737098775,\n",
       " 8: 0.5350838647979775,\n",
       " 9: -1.0564039061722008,\n",
       " 10: -1.3157302333011773,\n",
       " 11: -0.3146102147304947,\n",
       " 12: -0.3598493670689786,\n",
       " 13: 1.835965298015743,\n",
       " 14: -2.4398018320030297,\n",
       " 15: -2.6691522295796495,\n",
       " 16: -0.769265800957769,\n",
       " 17: -0.2809289274633179,\n",
       " 18: -0.44711740351701545,\n",
       " 19: -0.4115057631515958,\n",
       " 20: -0.6904238091413731,\n",
       " 21: -1.6366186006084846,\n",
       " 22: -0.9883151817260337,\n",
       " 23: -1.693230812134142,\n",
       " 24: 1.3842038501126206,\n",
       " 25: 0.4146284200472404,\n",
       " 26: 1.551147556682022,\n",
       " 27: 0.7102067244326817,\n",
       " 28: -0.14875862891887368,\n",
       " 29: 0.33930395288798854,\n",
       " 30: -1.0827971727532697,\n",
       " 31: -0.29138647511191923,\n",
       " 32: -0.5662955658010885,\n",
       " 33: 0.26323983924627314,\n",
       " 34: 0.6030880479206597,\n",
       " 35: 0.7728400592567305,\n",
       " 36: 0.2604854410886021,\n",
       " 37: -1.0264310220642034,\n",
       " 38: -0.05921784772444262,\n",
       " 39: 1.1087413248525484,\n",
       " 40: -0.6776165391314444,\n",
       " 41: 0.15785451586546878,\n",
       " 42: 0.20677315640058552,\n",
       " 43: 0.2433418413106078,\n",
       " 44: 0.027608166439223675,\n",
       " 45: 0.08800508368184204,\n",
       " 46: -0.023462546644142334,\n",
       " 47: -0.5838426252049181,\n",
       " 48: -0.5551161899130667,\n",
       " 49: -0.5842512532797984,\n",
       " 50: 0.0704244153399725,\n",
       " 51: -0.10087838232450701,\n",
       " 52: -0.861364182168856,\n",
       " 53: 0.9095093452461432,\n",
       " 54: 1.2426672427990388,\n",
       " 55: 3.152065854098226,\n",
       " 56: -0.35479862167367915,\n",
       " 57: -0.49299771745152043,\n",
       " 58: -1.708038832308867,\n",
       " 59: -1.7004439963118057,\n",
       " 60: 0.8492353421025357,\n",
       " 61: -1.2703208156908457,\n",
       " 62: -0.8463989588491067,\n",
       " 63: 1.9742286400322804,\n",
       " 64: -0.63253813315884,\n",
       " 65: -3.215791678552194,\n",
       " 66: 0.0697531736597898,\n",
       " 67: 1.8462500409152183,\n",
       " 68: -0.47767121176845767,\n",
       " 69: -1.2562541616921654,\n",
       " 70: -0.6588906031343718,\n",
       " 71: 0.2636091046835995,\n",
       " 72: -0.5448365176976957,\n",
       " 73: 1.3938328588731879,\n",
       " 74: -1.3213292475671683,\n",
       " 75: 0.58359434278104,\n",
       " 76: -0.7391316374988329,\n",
       " 77: 1.5900414576057067,\n",
       " 78: -0.053469614823528376,\n",
       " 79: -1.2015907346358772,\n",
       " 80: -1.45318981586682,\n",
       " 81: -1.5416766856411497,\n",
       " 82: 1.10397155270012,\n",
       " 83: 1.207182576386734,\n",
       " 84: 0.17448551466204942,\n",
       " 85: 0.5820540686145038,\n",
       " 86: 1.952859657140918,\n",
       " 87: 0.8573096343715021,\n",
       " 88: 2.0134841511487562,\n",
       " 89: 0.6747609132028743,\n",
       " 90: -0.3135624892844899,\n",
       " 91: -1.8250497812565818,\n",
       " 92: -2.309810658902804,\n",
       " 93: -0.7720271821576876,\n",
       " 94: 0.6318232413036996,\n",
       " 95: -0.625908796643425,\n",
       " 96: 0.45080895336646226,\n",
       " 97: -0.2061864684229292,\n",
       " 98: 1.875492392377339,\n",
       " 99: -1.1270640982117262}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1.2868288212895018,\n",
       " 1: -1.5126510426071544,\n",
       " 2: -2.253987605480358,\n",
       " 3: 0.4141407327558281,\n",
       " 4: 0.39502675672171333,\n",
       " 5: -0.5344973280069945,\n",
       " 6: 0.3334290803877628,\n",
       " 7: -0.45700123737098775,\n",
       " 8: 0.5350838647979775,\n",
       " 9: -1.0564039061722008,\n",
       " 10: -1.3157302333011773,\n",
       " 11: -0.3146102147304947,\n",
       " 12: -0.3598493670689786,\n",
       " 13: 1.835965298015743,\n",
       " 14: -2.4398018320030297,\n",
       " 15: -2.6691522295796495,\n",
       " 16: -0.769265800957769,\n",
       " 17: -0.2809289274633179,\n",
       " 18: -0.44711740351701545,\n",
       " 19: -0.4115057631515958,\n",
       " 20: -0.6904238091413731,\n",
       " 21: -1.6366186006084846,\n",
       " 22: -0.9883151817260337,\n",
       " 23: -1.693230812134142,\n",
       " 24: 1.3842038501126206,\n",
       " 25: 0.4146284200472404,\n",
       " 26: 1.551147556682022,\n",
       " 27: 0.7102067244326817,\n",
       " 28: -0.14875862891887368,\n",
       " 29: 0.33930395288798854,\n",
       " 30: -1.0827971727532697,\n",
       " 31: -0.29138647511191923,\n",
       " 32: -0.5662955658010885,\n",
       " 33: 0.26323983924627314,\n",
       " 34: 0.6030880479206597,\n",
       " 35: 0.7728400592567305,\n",
       " 36: 0.2604854410886021,\n",
       " 37: -1.0264310220642034,\n",
       " 38: -0.05921784772444262,\n",
       " 39: 1.1087413248525484,\n",
       " 40: -0.6776165391314444,\n",
       " 41: 0.15785451586546878,\n",
       " 42: 0.20677315640058552,\n",
       " 43: 0.2433418413106078,\n",
       " 44: 0.027608166439223675,\n",
       " 45: 0.08800508368184204,\n",
       " 46: -0.023462546644142334,\n",
       " 47: -0.5838426252049181,\n",
       " 48: -0.5551161899130667,\n",
       " 49: -0.5842512532797984,\n",
       " 50: 0.0704244153399725,\n",
       " 51: -0.10087838232450701,\n",
       " 52: -0.861364182168856,\n",
       " 53: 0.9095093452461432,\n",
       " 54: 1.2426672427990388,\n",
       " 55: 3.152065854098226,\n",
       " 56: -0.35479862167367915,\n",
       " 57: -0.49299771745152043,\n",
       " 58: -1.708038832308867,\n",
       " 59: -1.7004439963118057,\n",
       " 60: 0.8492353421025357,\n",
       " 61: -1.2703208156908457,\n",
       " 62: -0.8463989588491067,\n",
       " 63: 1.9742286400322804,\n",
       " 64: -0.63253813315884,\n",
       " 65: -3.215791678552194,\n",
       " 66: 0.0697531736597898,\n",
       " 67: 1.8462500409152183,\n",
       " 68: -0.47767121176845767,\n",
       " 69: -1.2562541616921654,\n",
       " 70: -0.6588906031343718,\n",
       " 71: 0.2636091046835995,\n",
       " 72: -0.5448365176976957,\n",
       " 73: 1.3938328588731879,\n",
       " 74: -1.3213292475671683,\n",
       " 75: 0.58359434278104,\n",
       " 76: -0.7391316374988329,\n",
       " 77: 1.5900414576057067,\n",
       " 78: -0.053469614823528376,\n",
       " 79: -1.2015907346358772,\n",
       " 80: -1.45318981586682,\n",
       " 81: -1.5416766856411497,\n",
       " 82: 1.10397155270012,\n",
       " 83: 1.207182576386734,\n",
       " 84: 0.17448551466204942,\n",
       " 85: 0.5820540686145038,\n",
       " 86: 1.952859657140918,\n",
       " 87: 0.8573096343715021,\n",
       " 88: 2.0134841511487562,\n",
       " 89: 0.6747609132028743,\n",
       " 90: -0.3135624892844899,\n",
       " 91: -1.8250497812565818,\n",
       " 92: -2.309810658902804,\n",
       " 93: -0.7720271821576876,\n",
       " 94: 0.6318232413036996,\n",
       " 95: -0.625908796643425,\n",
       " 96: 0.45080895336646226,\n",
       " 97: -0.2061864684229292,\n",
       " 98: 1.875492392377339,\n",
       " 99: -1.1270640982117262}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X).loc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**建模型**\n",
    "\n",
    "我们可以看到主要改变的参数是`num_factors`和`tasks`，读者可以想想为什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset of 0.01 of training for adaptive regularization\n",
      "-- Epoch 1\n",
      "Training log loss: 2.12467\n",
      "-- Epoch 2\n",
      "Training log loss: 1.74185\n",
      "-- Epoch 3\n",
      "Training log loss: 1.42232\n",
      "-- Epoch 4\n",
      "Training log loss: 1.16085\n",
      "-- Epoch 5\n",
      "Training log loss: 0.94964\n",
      "-- Epoch 6\n",
      "Training log loss: 0.78052\n",
      "-- Epoch 7\n",
      "Training log loss: 0.64547\n",
      "-- Epoch 8\n",
      "Training log loss: 0.53758\n",
      "-- Epoch 9\n",
      "Training log loss: 0.45132\n",
      "-- Epoch 10\n",
      "Training log loss: 0.38187\n"
     ]
    }
   ],
   "source": [
    "fm = pylibfm.FM(num_factors=50, num_iter=10, verbose=True, task=\"classification\", initial_learning_rate=0.0001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于是分类任务，误差函数肯定不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation log loss: 1.3678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Validation log loss: %.4f\" % log_loss(y_test,fm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
