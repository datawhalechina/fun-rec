# 3.1 机器学习相关

## 3.1.1 机器学习
- 介绍一个最熟悉的机器学习算法

- 决策树怎么建树，基尼系数公式

- Adaboost拟合目标是什么

- Adaboost介绍一下，每个基学习器的权重怎么得到的

- 介绍下GBDT

- 介绍XGBoost

- 介绍下LightGBM

- LightGBM相对于XGBoost的改进

- GBDT中的梯度是什么，怎么用

- GBDT如何计算特征重要性

- GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差

- 介绍XGBoost中的并行

- 介绍XGBoost中精确算法与近似算法

- XGBoost如何处理空缺值，为何要进行行采样、列采样

- 讲一下xgboost算法，xgboost是如何处理离散特征的，xgb怎么训练，xgb算法优点，怎么选特征，主要参数有哪些，xgb的特征重要性怎么看

- xgboost介绍一下，xgb对目标函数二阶泰勒展开，哪个是x，哪个是delta x, 一阶导和二阶导是对谁求得

- 为什么高维稀疏数据，LR比GBDT要好

- 随机森林与GBDT采样的区别

- 随机森林中列采样的作用

- bagging与boosting对比, boosting和bagging的区别及分别适用于什么场景

- bagging与boosting分别从什么角度降低过拟合

- 逻辑回归如何避免过拟合

- 推导逻辑回归损失函数和损失函数求导

- 正则化项L1和L2为什么有用

- l1正则不可导，如何优化

- 什么样的特征容易产生比较小的权重

- 随机森林采样n次，n趋于无穷大，oob样本的概率接近于？

- 逻辑回归与树模型的优缺点

- 对于高维稀疏数据，树模型能训练吗？一般怎么处理

- 树模型一般有哪些参数，分别有什么作用

- 随机森林如何处理空缺值

- 介绍kmeans，与其他聚类算法的对比

- 机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？

- 如何解决过拟合问题？哪些角度

- LR的原理，问题的假设，为什么用交叉熵作为损失函数

- LR损失函数写一下

- LR是不是凸优化问题，如何判断LR达到最优值

- LR一般用什么数据，一般有什么特点(离散数据，离散化的一堆优点)

- LR，SVM, xgboost如何防止过拟合

- lr和树模型，离散特征和连续特征分别怎么处理

- lr和线性回归的区别

- 连续特征可以直接输入到lr中不? （归一化和标准化有什么区别）

- 线性回归可以求闭式解，逻辑回归可以吗，为什么，LR用什么求解参数，为什么要用梯度下降算法

- SVM和LR的区别

- SVM的公式会推导嘛，SVM的损失函数

- SVM原理，为什么求最大间隔，为什么用核函数，常见的核函数及区别

- SVM支持向量怎么得到的

- 写一下SVM的原问题和对偶问题，分别解释一下

- SVM核函数有什么性质，写一下SVM核化的形式

- 无监督学习，半监督学习，有监督学习的区别

- 有哪些无监督学习的方法（kmeans,pca,生成模型，自编码器）

- 有哪些回归模型（多项式回归，树模型，svr, 神经网络）

- 生成模型、判别模型的区别

- 概率和似然的区别

- 最大似然估计和后验概率的区别，分别用LR来推导损失函数的话有什么区别（乘以W的先验概率）

- 朴素贝叶斯介绍，朴素贝叶斯公式，为什么朴素

- l1,l2特性及原理，分别适用于那些场合

- 给一个多峰数据场景，为什么l2不适合，可以怎么解决

- 讲讲Kmeans、EM算法

- 机器学习中怎么解决过拟合，DNN中怎么解决

- 说一下SVD怎么降维

- 推导softmax做激活函数求导

- LR，SVM,xgb哪个对样本不平衡不太敏感，顺便把SVM和xgb介绍了

- 降维方法了解嘛，PCA? 为什么取特征值前k大的对应的特征向量组成的矩阵？低秩表示

  

## 3.1.2 深度学习

- 梯度是什么，hessian矩阵怎么求
- 有没有上过凸优化的课程，如何判断凸函数
- 防止过拟合的策略有哪些
- dropout怎么防止过拟合, Dropout在训练和测试区别
- BN介绍，BN怎么防止过拟合，怎么用的，参数量, 参数怎么得到的
- 优化器，SGD与Adam的异同点
- 介绍一下你了解的激活函数以及优缺点
- 介绍一下深度学习的优化器发展史, 及常见的优化算法
- 梯度爆炸和梯度消失问题
- SGD缺点，已经有什么改进的优化器
- 网络权重初始化为0有什么影响，初始化为一个非0的常数呢？
- embedding如何设置维度？越大越好还是越小越好？
- transformer中计算attention除于根号d的作用
- embedding如何训练
- 介绍下attention，相比cnn、lstm的优势
- word2vec如何进行负采样
- word2vec两种训练方法的区别，具体损失函数
- 介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少
- transformer中encoder和decoder的输入分别是什么
- transformer中encoder与decoder的QKV矩阵如何产生
- transformer中QKV矩阵是否可以设置成同一个
- transformer与bert的位置编码有什么区别
- BERT中计算attention的公式
- BERT中LayerNorm的作用，为什么不用BN？
- BERT中的两种预训练任务介绍
- 深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的
- 激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？
- pagerank相比于tf-idf的优势
- 画一下LSTM的结构图
- RNN和LSTM的区别，解决了什么问题，为什么解决了梯度消失的问题
- 深度模型和传统机器学习模型对数据量的要求，什么场景用什么模型



## 3.1.3 特征工程
- 特征工程一般怎么做
- 特征数值分布比较稀疏如何处理
- 正负样本不均衡如何处理
- 连续特征离散化的作用
- 对id类特征onehot导致维度过高，如何处理？
- 如何进行特征筛选
- DNN能做特征交叉嘛
- 海量类别特征该如何处理，有什么方法
- pearson系数
- 归一化和标准化有什么区别
- 如果不使用最近邻检索的库，你会怎么做最近邻检索



## 3.1.4 评估指标

- auc的含义和计算方法, 有没有更快的计算方法
- AUC会不会出现小于0.5的情况，出现了怎么调bug
- AUC为1可能是由什么导致的？
- 分类评估指标中，F1和AUC有什么区别
- 分类指标用的什么，哪个分类指标对正负样本分布不敏感
- 如果对负样本进行采样，auc的计算结果会发生变化吗
- 交叉熵跟MSE有什么区别
- micro-f1解释
- 介绍下排序指标ndcg
- 回归指标应该用什么
- AUC和precision,recall，F1的区别，不同情况怎么选择指标
- Group auc了解嘛
- 给数据计算AUC
- 分类评价指标，TPR,FPR等的含义